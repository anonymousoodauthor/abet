{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and compare models on ID/OOD Perforamnce\n",
    "\n",
    "In this notebook, you can compare the differences between models and training and architecture paradigms. Given that the results for a model run such as a checkpoint and counts file are saved, different model configurations can be compared through graphs and visualizations of score distributions. Below, we provide code to compare a learned temperature model scored with AbeT with a normal model scored with Standardized Max Logit, Max Logit, Entropy, and Max Softmax Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install h5py Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_evaluation_setup\n",
    "from src.imageaugmentations import Compose, Normalize, ToTensor\n",
    "from evaluation import eval_pixels\n",
    "from viz_utils import twod_to_threed, get_crops, plot_curve, plot_curve_comparisons, plot_score_graphs\n",
    "from src.helper import counts_array_to_data_list\n",
    "from src.model_utils import inference\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ROOT ARGS\n",
    "args = {\n",
    "    'TRAINSET': None,\n",
    "    'VALSET': 'LostAndFound',\n",
    "    # 'VALSET': 'RoadAnomaly',\n",
    "    'split': 'test',\n",
    "    'MODEL': None,\n",
    "    'val_epoch': None,\n",
    "    'pareto_alpha': None,\n",
    "    'pixel_eval': True,\n",
    "    'segment_eval': False,\n",
    "    'temperature_model': \"none\",\n",
    "    'checkpoint': \"REQUIRE_CHECKPONT.PTH\",\n",
    "    'score_function': 'entropy',\n",
    "    'ood_finetune': 'FALSE',\n",
    "    'name': None,\n",
    "}\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "\n",
    "#########################\n",
    "# TEMPERATURE MODEL\n",
    "#########################\n",
    "these_args = args.copy()\n",
    "these_args.update({\n",
    "    \"temperature_model\": \"learned\",\n",
    "    \"checkpoint\": \" /your/path/to/Abet/weights/full_learned_temp_best.pth\",\n",
    "    \"score_function\": \"abet\",\n",
    "    \"name\": \"AbeT (Ours)\"\n",
    "})\n",
    "comparisons.append(these_args)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Comparison models and scoring methods: cityscapes_best.pth from https://github.com/NVIDIA/semantic-segmentation/tree/sdcnet\n",
    "###################################\n",
    "these_args = args.copy()\n",
    "these_args.update({\n",
    "    \"checkpoint\": \" /your/path/to/Abet/weights/cityscapes_best.pth\",\n",
    "    \"score_function\": \"sml\",\n",
    "    \"temperature_model\": \"none\",\n",
    "    \"name\": \"Standardized\\nMax Logits\"\n",
    "})\n",
    "comparisons.append(these_args)\n",
    "\n",
    "these_args = args.copy()\n",
    "these_args.update({\n",
    "    \"checkpoint\": \" /your/path/to/Abet/weights/cityscapes_best.pth\",\n",
    "    \"score_function\": \"max_logit\",\n",
    "    \"temperature_model\": \"none\",\n",
    "    \"name\": \"Max Logit\"\n",
    "})\n",
    "comparisons.append(these_args)\n",
    "\n",
    "these_args = args.copy()\n",
    "these_args.update({\n",
    "    \"checkpoint\": \" /your/path/to/Abet/weights/cityscapes_best.pth\",\n",
    "    \"score_function\": \"entropy\",\n",
    "    \"temperature_model\": \"none\",\n",
    "    \"name\": \"Entropy\"\n",
    "})\n",
    "comparisons.append(these_args)\n",
    "\n",
    "these_args = args.copy()\n",
    "these_args.update({\n",
    "    \"checkpoint\": \" /your/path/to/Abet/weights/cityscapes_best.pth\",\n",
    "    \"score_function\": \"msp\",\n",
    "    \"temperature_model\": \"none\",\n",
    "    \"name\": \"MSP\"\n",
    "})\n",
    "comparisons.append(these_args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationRun:\n",
    "    def __init__(self, name, config, args, datloader, run_name_str) -> None:\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "        self.args = args\n",
    "        self.run_name_str = run_name_str\n",
    "        self.datloader = datloader\n",
    "        \n",
    "        self.eval_pixels = eval_pixels(\n",
    "            self.config.params,\n",
    "            self.config.roots,\n",
    "            self.config.dataset,\n",
    "            self.args,\n",
    "            run_name_str=self.run_name_str,\n",
    "        ) # params, roots, dataset, args, run_name_str\n",
    "        self.inference = inference(self.config.params, self.config.roots, self.datloader, self.datloader.num_eval_classes, self.run_name_str)\n",
    "        self.root_path = self.eval_pixels.save_path_data\n",
    "        self.scores_dict = json.load(open(os.path.join(self.root_path, \"data.json\"), \"r\"))        \n",
    "        if \"threshold\" not in self.scores_dict:\n",
    "            ind = (np.abs(np.array(self.scores_dict[\"roc_tpr\"]) - 0.95)).argmin()\n",
    "            self.scores_dict[\"threshold\"] = self.scores_dict[\"roc_thresholds\"][ind]\n",
    "        self.threshold = self.scores_dict[\"threshold\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_eval_runs = dict()\n",
    "\n",
    "for comparison_args in comparisons:\n",
    "    print(\"=\" * 50)\n",
    "    config = config_evaluation_setup(comparison_args)\n",
    "    config.params.temperature_model = comparison_args[\"temperature_model\"]\n",
    "    config.params.checkpoint = comparison_args[\"checkpoint\"]\n",
    "    run_name_str = f\"valset_{comparison_args['VALSET']}_\" + \\\n",
    "                    f\"split_{comparison_args['split']}_\" + \\\n",
    "                    f\"{comparison_args['temperature_model']}_temperature_model_\" + \\\n",
    "                    f\"{comparison_args['score_function']}_\" + \\\n",
    "                    f\"OODFT_{comparison_args['ood_finetune']}\" + \\\n",
    "                    f\"_ckpt_{config.params.checkpoint.split('/')[-1][:-4]}\"\n",
    "                    # (f\"_ckpt_{config.params.checkpoint.split('/')[-1][:-4]}\" if \"learned\" in comparison_args['temperature_model'] else \"\")\n",
    "    print(run_name_str)\n",
    "                \n",
    "\n",
    "    transform = Compose([ToTensor(), Normalize(config.dataset.mean, config.dataset.std)])\n",
    "    datloader = config.dataset(\n",
    "        root=config.roots.eval_dataset_root, transform=transform, split=comparison_args[\"split\"]\n",
    "    )\n",
    "\n",
    "    # ASSERT SCORING FUNCTION WORKS WITH MODEL TYPE\n",
    "    if comparison_args[\"score_function\"] in [\"energy\", \"godin\"]:\n",
    "        assert \"learned\" == comparison_args[\"temperature_model\"]\n",
    "    eval_run = EvaluationRun(comparison_args['name'], config, comparison_args, datloader, run_name_str)\n",
    "    comparison_eval_runs[eval_run.name] = eval_run\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce our figure in the AbeT paper, evaluate the results on indices 80 and 1117 in LostAndFound and 3 and 23 in RoadAnomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "eval_inds = np.arange(4)\n",
    "print(eval_inds)\n",
    "\n",
    "im_paths = []\n",
    "name_to_scores = defaultdict(list)\n",
    "name_to_labels = defaultdict(list)\n",
    "\n",
    "for i, (name, eval_run) in enumerate(comparison_eval_runs.items()):\n",
    "    for eval_ind in eval_inds:\n",
    "        outputs, gt_train, gt_label, im_path = eval_run.inference.probs_gt_load(eval_ind)\n",
    "        if np.sum(gt_train == datloader.train_id_out) < 50:\n",
    "            continue\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, numerators, temperatures = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "            nummerators, temperatures = None, None\n",
    "        scores = eval_run.eval_pixels.score_fn(logits, numerators, temperatures, datloader.num_eval_classes)\n",
    "        name_to_scores[name].append(scores)\n",
    "        name_to_labels[name].append(gt_train)\n",
    "        \n",
    "        if i == 0:\n",
    "            im_paths.append(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crops(image, train_labels, scores, ood_ind=2, border=50):\n",
    "    IMW, IMH = train_labels.shape\n",
    "    xx, yy = np.where(train_labels == ood_ind)\n",
    "    minx = max(0, np.min(xx) -border)\n",
    "    maxx = min(np.max(xx) + border, IMW)\n",
    "    miny = max(0, np.min(yy) - border)\n",
    "    maxy = min(np.max(yy) + border, IMH)\n",
    "    \n",
    "    # square crops\n",
    "    # if maxx - minx > maxy - miny:\n",
    "    #     half_dist = (maxx - minx) / 2.0\n",
    "    #     middle_y = (maxy + miny) / 2.0\n",
    "    #     maxy = min(int(middle_y + half_dist), IMH)\n",
    "    #     miny = max(int(middle_y - half_dist), 0)\n",
    "    # else:\n",
    "    #     half_dist = (maxy - miny) / 2.0\n",
    "    #     middle_x = (maxx + minx) / 2.0\n",
    "    #     maxx = min(int(middle_x + half_dist), IMW)\n",
    "    #     minx = max(int(middle_x - half_dist), 0)\n",
    "    \n",
    "    # need rectangular crops 1.777777 for LAF to match RA\n",
    "    x_width = maxx - minx\n",
    "    y_new_width = x_width / 2.0 * 1.7777777\n",
    "    y_center = (maxy + miny) / 2.0\n",
    "    maxy = int(y_center + y_new_width)\n",
    "    miny = int(y_center - y_new_width)\n",
    "    return image[minx: maxx, miny:maxy, :], train_labels[minx:maxx, miny:maxy], scores[minx:maxx, miny:maxy]\n",
    "\n",
    "def get_rgb_norm_scores(scores, train_labels, in_id, out_id, thresh, bottom_clip=0, top_clip=1):\n",
    "    rgb_scores = twod_to_threed(np.zeros_like(scores).astype(float))\n",
    "    scores = np.clip(scores - thresh, 0, np.max(scores))\n",
    "    norm_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
    "    rgb_scores[:, :, 0] = norm_scores\n",
    "    # rgb_scores = twod_to_threed(norm_scores) # for white masks\n",
    "    rgb_scores[(train_labels != in_id) &(train_labels != out_id)] = [0,0,0]  # set VOID to black\n",
    "    \n",
    "    return rgb_scores\n",
    "\n",
    "def get_rgb_labels(train_labels, in_id, out_id):\n",
    "    rgb_labels = twod_to_threed(np.zeros_like(train_labels, dtype=np.uint8))\n",
    "    rgb_labels[train_labels == out_id] =[255,0,0] \n",
    "    # rgb_labels[train_labels == out_id] = [255, 255, 255] # white masks\n",
    "    return rgb_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from src.calc import calc_precision_recall, calc_sensitivity_specificity, get_tpr95_ind\n",
    "from src.helper import counts_array_to_data_list\n",
    "\n",
    "def run_eval(scores, labels, thresh, datloader):\n",
    "    data = {\"in\": np.zeros(100), \"out\": np.zeros(100)}\n",
    "    bins = np.linspace(0, 1, 101)\n",
    "    \n",
    "    in_scores = scores[labels == datloader.train_id_in]\n",
    "    out_scores = scores[labels == datloader.train_id_out]\n",
    "\n",
    "    in_mean, in_std = np.mean(in_scores), np.std(in_scores)\n",
    "    data[\"in\"] += np.histogram(in_scores, bins=bins, density=False)[0]\n",
    "\n",
    "    if len(out_scores) > 0:\n",
    "        out_mean, out_std = np.mean(out_scores), np.std(out_scores)\n",
    "        data[\"out\"] += np.histogram(out_scores, bins=bins, density=False)[0]\n",
    "        roc_fpr, roc_tpr, roc_thresholds, auroc = calc_sensitivity_specificity(data, balance=True)\n",
    "        roc_stats = dict(fprs=roc_fpr, tprs=roc_tpr, thresholds=roc_thresholds, auroc=auroc)\n",
    "        pr_precision, pr_recall, pr_thresholds, auprc = calc_precision_recall(data)\n",
    "        ind, tpr95, fpr95 = get_tpr95_ind(roc_fpr, roc_tpr)\n",
    "        pr_stats = dict(precisions=pr_precision, recalls=pr_recall, thresholds=pr_thresholds, auprc=auprc)\n",
    "    else:\n",
    "        roc_stats, pr_stats = None, None, None\n",
    "    return roc_stats, pr_stats, fpr95, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_data_to_lists(data):\n",
    "    ratio_in = np.sum(data[\"in\"]) / (np.sum(data[\"in\"]) + np.sum(data[\"out\"]))\n",
    "    ratio_out = 1 - ratio_in\n",
    "    x1 = counts_array_to_data_list(np.array(data[\"in\"]), 1e7 * ratio_in)\n",
    "    x2 = counts_array_to_data_list(np.array(data[\"out\"]), 1e7 * ratio_out)\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "def plot_counts_histogram(data, ax, num_bins, thresh):\n",
    "    bins = np.linspace(0, 100, num_bins + 1)\n",
    "    x1, x2 = counts_data_to_lists(data)\n",
    "    ax.hist(\n",
    "        [x1, x2],\n",
    "        bins,\n",
    "        label=[\"ID\", \"OOD\"],\n",
    "        weights=[np.ones(len(x1)) / len(x1), np.ones(len(x2)) / len(x2)],\n",
    "    )\n",
    "    if thresh is not None:\n",
    "        ax.axvline(thresh * 100, color=\"black\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_crop = \"F\" in comparisons[0]['VALSET']\n",
    "show_hist = False # show histogram of scores\n",
    "show_captions = False # print the scores for each image and model\n",
    "f, ax = plt.subplots(len(eval_inds) + show_hist, len(name_to_scores) + 1 + show_hist, figsize=(12, 8))\n",
    "\n",
    "# cols\n",
    "for j, name in enumerate(name_to_scores.keys()):\n",
    "    scores_list = name_to_scores[name]\n",
    "    labels_list = name_to_labels[name] \n",
    "         \n",
    "    # rows\n",
    "    for i in range(len(scores_list)):\n",
    "        print(i)\n",
    "        scores = scores_list[i]\n",
    "        labels = labels_list[i]\n",
    "        im_path = im_paths[i]\n",
    "        im = np.array(Image.open(im_path).convert(\"RGB\")) # [0,255]\n",
    "        thresh = comparison_eval_runs[name].threshold\n",
    "        rgb_norm_scores = get_rgb_norm_scores(scores, labels, in_id=datloader.train_id_in, out_id=datloader.train_id_out, thresh=thresh, bottom_clip=0, top_clip=np.max(scores)) \n",
    "        rgb_labels = get_rgb_labels(labels, in_id=datloader.train_id_in, out_id=datloader.train_id_out)\n",
    "        # full_roc_stats, full_pr_stats, full_fpr95, full_data = run_eval(scores, labels, thresh, datloader)\n",
    "        \n",
    "        if use_crop:\n",
    "            cropped_im, cropped_labels, cropped_scores = get_crops(im, labels, scores, ood_ind=datloader.train_id_out, border=40)\n",
    "            cropped_rgb_norm_scores = get_rgb_norm_scores(cropped_scores, cropped_labels,in_id=datloader.train_id_in, out_id=datloader.train_id_out, thresh=thresh, bottom_clip=0, top_clip=np.max(scores)) \n",
    "            cropped_rgb_labels = get_rgb_labels(cropped_labels, in_id=datloader.train_id_in, out_id=datloader.train_id_out)\n",
    "        \n",
    "        ax[i, j + 1+ show_hist].imshow(cropped_rgb_norm_scores if use_crop else rgb_norm_scores)\n",
    "        # if show_captions:\n",
    "            # cropped_caption = f\"\\ncropped AUROC {cropped_roc_stats['auroc']:.4f}\\ncropped AUPRC {cropped_pr_stats['auprc']:.4f}\\nfpr95 {cropped_fpr95:.4f}\"\n",
    "            # full_caption =  f\"AUROC {full_roc_stats['auroc']:.4f}\\nAUPRC {full_pr_stats['auprc']:.4f}\\nfpr95 {full_fpr95:.4f}\"\n",
    "            # ax[i, j+1 + show_hist].set_xlabel(full_caption)\n",
    "        \n",
    "        if j == 0: \n",
    "            if not use_crop:\n",
    "                cropped_im = im\n",
    "                cropped_labels = labels\n",
    "            masked_im = cropped_im.copy() / 255.\n",
    "            ood_delta = np.where(cropped_labels == datloader.train_id_out, 0.2, 0.0)\n",
    "\n",
    "            masked_im[:, :, 0] += ood_delta\n",
    "            masked_im[:, :, 1] -= ood_delta\n",
    "            masked_im[:, :, 2] -= ood_delta\n",
    "            \n",
    "            masked_im[(cropped_labels != datloader.train_id_in) &(cropped_labels != datloader.train_id_out)] = [0,0,0]  \n",
    "            masked_im = np.clip(masked_im, 0, 1)\n",
    "            ax[i, 0].imshow(masked_im)\n",
    "            if show_hist:\n",
    "                cropped_roc_stats, cropped_pr_stats, cropped_fpr95, cropped_data = run_eval(cropped_scores, cropped_labels, thresh, datloader)\n",
    "                plot_counts_histogram(cropped_data, ax[i, 1], 100, thresh)\n",
    "            \n",
    "       \n",
    "for i, row_ax in enumerate(ax):\n",
    "    for j, row_col_ax in enumerate(row_ax):\n",
    "        if j == 1 and show_hist:\n",
    "            continue\n",
    "        row_col_ax.tick_params(axis='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        for loc, spine in row_col_ax.spines.items():\n",
    "            spine.set_color(\"white\")\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "plt.margins(0,0)\n",
    "ax[-1, 0].set_xlabel(f\"Image with \\nOOD Label\", fontweight=\"bold\", fontsize=\"x-large\")\n",
    "if show_hist:\n",
    "    ax[-1, 1].set_xlabel(f\"counts hist\")\n",
    "for j, (name, eval_run) in enumerate(comparison_eval_runs.items()):\n",
    "    ax[-1, j + 1 + show_hist].set_xlabel(f\"{eval_run.name}\", fontweight=\"bold\", fontsize=\"x-large\")\n",
    "\n",
    "all_names_str = \"_\".join(name_to_scores.keys()).replace(\" \", \"_\").replace('\\n', '')\n",
    "dataset_str = f\"{comparisons[0]['VALSET']}_{comparisons[0]['split']}\"\n",
    "save_dir = f\"/your/path/to/Abet/io/viz_plots/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"{all_names_str}_{dataset_str}_matrix_viz.png\")\n",
    "plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "print(f\"saved to {save_path}\")\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('3.8.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd27734643b4e06490511230ad4c1bd973c5f6751355b808f47a0f4251657679"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
